{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641c0109-ebdb-45cd-a2ee-84b64495bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "import scipy as sp\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425b5577-3c95-4500-888a-370e4d6d9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and preprocess Boston Housing data\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "X = boston.data.values\n",
    "y = boston.target.values\n",
    "\n",
    "# Normalize features and target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9d0cbc-ba49-45f7-9908-d79f4b45aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True, threshold=2, use_mask=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.threshold = -torch.log10(torch.tensor(threshold))\n",
    "        self.use_mask = use_mask\n",
    "        self.entropy = None\n",
    "        \n",
    "    def wval(self, M):\n",
    "        L = M.shape[1] if M.ndim == 2 else M.shape[1] * M.shape[2] * M.shape[3]\n",
    "        alpha = torch.as_tensor(1/2, dtype=M.dtype, device=M.device)\n",
    "        beta = torch.as_tensor((L-1)/2, dtype=M.dtype, device=M.device)\n",
    "    \n",
    "        # Normalize each weight vector going from a node in A to layer B\n",
    "        M_normed = F.normalize(M, p=2, dim=1)\n",
    "        \n",
    "        M_clamped = torch.clamp(M_normed**2, min=1e-8, max=1-1e-8)\n",
    "    \n",
    "        # Compute the incomplete beta function manually using Beta distribution\n",
    "        # This is done using a formula for the Beta CDF (this is a simple approximation)\n",
    "        B = torch.exp(\n",
    "            torch.lgamma(alpha + beta) - torch.lgamma(alpha) - torch.lgamma(beta)\n",
    "        )  # Beta function normalization constant\n",
    "        cdf = (M_clamped**(alpha - 1)) * ((1 - M_clamped)**(beta - 1)) / B\n",
    "        beta_surv = 1 - cdf  # Survival function\n",
    "        \n",
    "        w_val = -torch.log10(beta_surv)\n",
    "        assert w_val.shape == M.shape\n",
    "\n",
    "        return w_val\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.use_mask:\n",
    "            # Compute significance\n",
    "            significance = self.wval(self.weight)\n",
    "    \n",
    "            # Create binary mask\n",
    "            mask = (significance >= self.threshold).float()\n",
    "    \n",
    "            # Apply the mask (zero out insignificant weights)\n",
    "            masked_weight = self.weight * mask\n",
    "        else:\n",
    "            masked_weight = self.weight\n",
    "\n",
    "        flat_M = torch.flatten(masked_weight)\n",
    "        flat_M_2 = flat_M.pow(2)\n",
    "        self.entropy = sp.stats.entropy(flat_M_2.detach().numpy())\n",
    "            \n",
    "        return F.linear(input, masked_weight, self.bias)\n",
    "\n",
    "\n",
    "\n",
    "class FlexibleMLP(nn.Module):\n",
    "    def __init__(self, input_size=13, layer1=64, layer2=32, activation_fn=nn.ReLU, use_mask=True, threshold=2):\n",
    "        super().__init__()\n",
    "        # input_size=13\n",
    "        # layer1=64\n",
    "        # layer2=32\n",
    "        \n",
    "        # Instantiate activation function\n",
    "        self.activation = activation_fn()\n",
    "\n",
    "        # Build model\n",
    "        self.model = nn.Sequential(\n",
    "            MaskedLinear(input_size, layer1, threshold=threshold, use_mask=use_mask),\n",
    "            self.activation,\n",
    "            MaskedLinear(layer1, layer2, threshold=threshold, use_mask=use_mask),\n",
    "            self.activation,\n",
    "            MaskedLinear(layer2, 1, threshold=threshold, use_mask=use_mask)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6eecf1-55fd-49c7-99ca-19cca71683ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.2901, Test Loss: 0.4396\n",
      "Epoch 10, Train Loss: 0.5453, Test Loss: 0.2876\n",
      "Epoch 20, Train Loss: 0.4358, Test Loss: 0.2613\n",
      "Epoch 30, Train Loss: 0.3516, Test Loss: 0.2419\n",
      "Epoch 40, Train Loss: 0.3040, Test Loss: 0.2247\n",
      "Epoch 50, Train Loss: 0.2716, Test Loss: 0.2136\n",
      "Epoch 60, Train Loss: 0.2482, Test Loss: 0.1942\n",
      "Epoch 70, Train Loss: 0.2324, Test Loss: 0.1729\n",
      "Epoch 80, Train Loss: 0.2224, Test Loss: 0.1585\n",
      "Epoch 90, Train Loss: 0.2160, Test Loss: 0.1507\n",
      "Epoch 0, Train Loss: 1.4419, Test Loss: 0.9240\n",
      "Epoch 10, Train Loss: 0.9959, Test Loss: 0.7086\n",
      "Epoch 20, Train Loss: 0.7073, Test Loss: 0.4369\n",
      "Epoch 30, Train Loss: 0.5504, Test Loss: 0.3199\n",
      "Epoch 40, Train Loss: 0.4839, Test Loss: 0.2755\n",
      "Epoch 50, Train Loss: 0.4457, Test Loss: 0.2702\n",
      "Epoch 60, Train Loss: 0.4280, Test Loss: 0.2668\n",
      "Epoch 70, Train Loss: 0.4174, Test Loss: 0.2660\n",
      "Epoch 80, Train Loss: 0.4111, Test Loss: 0.2625\n",
      "Epoch 90, Train Loss: 0.4072, Test Loss: 0.2622\n",
      "Epoch 0, Train Loss: 1.4699, Test Loss: 0.7706\n",
      "Epoch 10, Train Loss: 0.5361, Test Loss: 0.2506\n",
      "Epoch 20, Train Loss: 0.3615, Test Loss: 0.1827\n",
      "Epoch 30, Train Loss: 0.2829, Test Loss: 0.1537\n",
      "Epoch 40, Train Loss: 0.2310, Test Loss: 0.1443\n",
      "Epoch 50, Train Loss: 0.2016, Test Loss: 0.1389\n",
      "Epoch 60, Train Loss: 0.1821, Test Loss: 0.1373\n",
      "Epoch 70, Train Loss: 0.1700, Test Loss: 0.1370\n",
      "Epoch 80, Train Loss: 0.1624, Test Loss: 0.1351\n",
      "Epoch 90, Train Loss: 0.1582, Test Loss: 0.1339\n"
     ]
    }
   ],
   "source": [
    "t = 2\n",
    "activations = [nn.Tanh, nn.Sigmoid, nn.ReLU]\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "models = []\n",
    "the_lambda = 0\n",
    "\n",
    "for act in activations:\n",
    "    test_loss = []\n",
    "    train_loss = []\n",
    "    model = FlexibleMLP(threshold=t, activation_fn=act, use_mask=False) # MyActivation\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # 4. Train\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = criterion(output, y_train)\n",
    "\n",
    "        if the_lambda > 0:\n",
    "            norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss += the_lambda*norm\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = criterion(model(X_test), y_test).item()\n",
    "            print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {val_loss:.4f}\")\n",
    "            test_loss.append([val_loss])\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "    test_losses.append(test_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c089db-8e94-4134-aff6-0e92158ad6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FlexibleMLP(\n",
       "   (activation): Tanh()\n",
       "   (model): Sequential(\n",
       "     (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "     (1): Tanh()\n",
       "     (2): MaskedLinear(in_features=64, out_features=32, bias=True)\n",
       "     (3): Tanh()\n",
       "     (4): MaskedLinear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " FlexibleMLP(\n",
       "   (activation): Sigmoid()\n",
       "   (model): Sequential(\n",
       "     (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "     (1): Sigmoid()\n",
       "     (2): MaskedLinear(in_features=64, out_features=32, bias=True)\n",
       "     (3): Sigmoid()\n",
       "     (4): MaskedLinear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       " ),\n",
       " FlexibleMLP(\n",
       "   (activation): ReLU()\n",
       "   (model): Sequential(\n",
       "     (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "     (1): ReLU()\n",
       "     (2): MaskedLinear(in_features=64, out_features=32, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): MaskedLinear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017d7ad7-6a00-4a44-a108-3fb0b07006f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLinear(in_features=13, out_features=64, bias=True) 2.5678496\n",
      "MaskedLinear(in_features=64, out_features=32, bias=True) 2.7266998\n",
      "MaskedLinear(in_features=32, out_features=1, bias=True) 1.3361826\n",
      "[np.float32(2.5678496), np.float32(2.7266998), np.float32(1.3361826)]\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "layer_count = 0\n",
    "\n",
    "for name, layer in models[2].named_modules():\n",
    "    if isinstance(layer, (nn.Conv2d, MaskedLinear, nn.Linear)):\n",
    "        layer_count += 1\n",
    "        print(layer, layer.entropy)\n",
    "        entropies.append(layer.entropy)\n",
    "\n",
    "#print(f\"Epoch {epoch + 1}, Weight Entropies: {entropies}\")\n",
    "print(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c2bd2d-e7df-451d-a8fd-5d1147ce8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleMLPC(nn.Module):\n",
    "    def __init__(self, input_size=28*28, layer1=256, layer2=128, num_classes=10 ,activation_fn=nn.ReLU, use_mask=True, threshold=2):\n",
    "        super().__init__()\n",
    "        # input_size=13\n",
    "        # layer1=64\n",
    "        # layer2=32\n",
    "        \n",
    "        # Instantiate activation function\n",
    "        self.activation = activation_fn()\n",
    "\n",
    "        # Build model\n",
    "        self.model = nn.Sequential(\n",
    "            MaskedLinear(input_size, layer1, threshold=threshold, use_mask=use_mask),\n",
    "            self.activation,\n",
    "            MaskedLinear(layer1, layer2, threshold=threshold, use_mask=use_mask),\n",
    "            self.activation,\n",
    "            MaskedLinear(layer2, num_classes, threshold=threshold, use_mask=use_mask)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten from [B, 1, 28, 28] to [B, 784]\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9006a2e-88ee-4198-ab84-4ad71a797312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# 2. Load MNIST\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "input_dim1 = images.shape[1]*images.shape[2]*images.shape[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff508a43-c8b0-42b1-819e-ec19d1de1284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.3113\n",
      "Epoch 2, Loss: 0.2279\n",
      "Epoch 3, Loss: 0.2221\n",
      "Epoch 4, Loss: 0.1991\n",
      "Epoch 5, Loss: 0.2053\n",
      "Test Accuracy: 0.9584\n"
     ]
    }
   ],
   "source": [
    "# 3. Initialize model, loss, optimizer\n",
    "t = 6\n",
    "activations = [nn.ReLU]\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "models = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "the_lambda = 1e-6\n",
    "\n",
    "for act in activations:\n",
    "    model = FlexibleMLPC(threshold=t, input_size=input_dim1, activation_fn=act, use_mask=False).to(device)  # Swap MyActivation with nn.ReLU, nn.Tanh, etc.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 4. Training loop\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if the_lambda > 0:\n",
    "                norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += the_lambda*norm\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    models.append(model);\n",
    "    # 5. Evaluation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    print(f\"Test Accuracy: {correct / total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df70ae60-8f6a-44be-b835-8e2362d18064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLinear(in_features=784, out_features=256, bias=True) 10.796702\n",
      "MaskedLinear(in_features=256, out_features=128, bias=True) 8.649537\n",
      "MaskedLinear(in_features=128, out_features=10, bias=True) 5.943685\n",
      "[np.float32(10.796702), np.float32(8.649537), np.float32(5.943685)]\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "layer_count = 0\n",
    "\n",
    "for name, layer in models[0].named_modules():\n",
    "    if isinstance(layer, (nn.Conv2d, MaskedLinear, nn.Linear)):\n",
    "        layer_count += 1\n",
    "        print(layer, layer.entropy)\n",
    "        entropies.append(layer.entropy)\n",
    "\n",
    "#print(f\"Epoch {epoch + 1}, Weight Entropies: {entropies}\")\n",
    "print(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a919a35-57cd-4c11-ac90-f6876253ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.weight_norm as weight_norm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f5968b-1172-43b2-9c54-959166b54554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Data preprocessing\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))  # Mean and std of MNIST\n",
    "# ])\n",
    "\n",
    "# # Load datasets\n",
    "# train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "# Define transforms (including normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load training set\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Load test set\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cef146a-d5c9-4150-8809-13235aebe5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, activation_fn=nn.ReLU, use_mask=True):\n",
    "        super(CNN, self).__init__()\n",
    "        self.activation = activation_fn()\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "        # MNIST\n",
    "        # self.model = nn.Sequential(\n",
    "        #     nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "        #     self.activation,\n",
    "        #     nn.MaxPool2d(2),                            # 28x28 → 14x14\n",
    "\n",
    "        #     nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 14x14 → 14x14\n",
    "        #     self.activation,\n",
    "        #     nn.MaxPool2d(2),                              # 14x14 → 7x7\n",
    "\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(64 * 7 * 7, 128),\n",
    "        #     self.activation,\n",
    "        #     nn.Linear(128, 10)\n",
    "        # )\n",
    "\n",
    "        #CIFAR-10\n",
    "        self.model = nn.Sequential(\n",
    "            MaskedConv2d(3, 64, 3, padding=1, use_mask=self.use_mask), \n",
    "            # nn.Conv2d(3, 64, 3, padding=1),     # doubled from 32 to 64\n",
    "            self.activation,\n",
    "            # nn.Conv2d(64, 64, 3, padding=1),    # extra conv layer\n",
    "            # EntropyConv2d(64, 64, 3, padding=1), \n",
    "            # self.activation,\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            MaskedConv2d(64, 128, 3, padding=1, use_mask=self.use_mask),\n",
    "            # nn.Conv2d(64, 128, 3, padding=1),   # doubled from 64 to 128\n",
    "            self.activation,\n",
    "            # nn.Conv2d(128, 128, 3, padding=1),  # extra conv layer\n",
    "            # EntropyConv2d(128, 128, 3, padding=1),\n",
    "            # self.activation,\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            MaskedLinear(128 * 8 * 8 , 512, use_mask=self.use_mask),\n",
    "            # nn.Linear(128 * 8 * 8, 512),        # doubled from 256 to 512\n",
    "            self.activation,\n",
    "            # nn.Linear(512, 512),                # extra dense layer\n",
    "            # EntropyLinear(512, 512),\n",
    "            # self.activation,\n",
    "            MaskedLinear(512, 10, use_mask=self.use_mask)\n",
    "            # nn.Linear(512, 10)\n",
    "        )\n",
    "        # self.features = nn.Sequential(\n",
    "        #     MaskedConv2d(3, 64, 3, padding=1, use_mask=True),\n",
    "        #     self.activation,\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        #     MaskedConv2d(64, 128, 3, padding=1, use_mask=True),\n",
    "        #     self.activation,\n",
    "        #     nn.MaxPool2d(2, 2),\n",
    "        # )\n",
    "\n",
    "        # # # Calculate flattened size\n",
    "        # # with torch.no_grad():\n",
    "        # #     dummy_input = torch.zeros(1, 3, 32, 32)\n",
    "        # #     dummy_output = self.features(dummy_input)\n",
    "        # #     flattened_size = dummy_output.view(1, -1).shape[1]\n",
    "        # #     print(flattened_size)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Flatten(),\n",
    "        #     MaskedLinear(flattened_size, 512, use_mask=True),\n",
    "        #     self.activation,\n",
    "        #     MaskedLinear(512, 10, use_mask=True),\n",
    "        # )\n",
    "\n",
    "        # self.model = nn.Sequential(self.features, self.classifier)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bias=True, stride=1, padding=0, threshold=2, use_mask=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.threshold = -torch.log10(torch.tensor(threshold))\n",
    "        self.use_mask = use_mask\n",
    "        self.entropy = None\n",
    "        \n",
    "    def wval(self, M):\n",
    "        L = M.shape[1] if M.ndim == 2 else M.shape[1] * M.shape[2] * M.shape[3]\n",
    "        alpha = torch.as_tensor(1/2, dtype=M.dtype, device=M.device)\n",
    "        beta = torch.as_tensor((L-1)/2, dtype=M.dtype, device=M.device)\n",
    "    \n",
    "        # Normalize each weight vector going from a node in A to layer B\n",
    "        M_normed = F.normalize(M, p=2, dim=1)\n",
    "        \n",
    "        M_clamped = torch.clamp(M_normed**2, min=1e-8, max=1-1e-8)\n",
    "    \n",
    "        # Compute the incomplete beta function manually using Beta distribution\n",
    "        # This is done using a formula for the Beta CDF (this is a simple approximation)\n",
    "        B = torch.exp(\n",
    "            torch.lgamma(alpha + beta) - torch.lgamma(alpha) - torch.lgamma(beta)\n",
    "        )  # Beta function normalization constant\n",
    "        cdf = (M_clamped**(alpha - 1)) * ((1 - M_clamped)**(beta - 1)) / B\n",
    "        beta_surv = 1 - cdf  # Survival function\n",
    "        \n",
    "        w_val = -torch.log10(beta_surv)\n",
    "        assert w_val.shape == M.shape\n",
    "\n",
    "        return w_val\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(input.shape)\n",
    "        if self.use_mask:\n",
    "            # Compute significance\n",
    "            significance = self.wval(self.weight)\n",
    "    \n",
    "            # Create binary mask\n",
    "            mask = (significance >= self.threshold).float()\n",
    "    \n",
    "            # Apply the mask (zero out insignificant weights)\n",
    "            masked_weight = self.weight * mask\n",
    "        else:\n",
    "            masked_weight = self.weight\n",
    "\n",
    "        flat_M = torch.flatten(masked_weight)\n",
    "        flat_M_2 = flat_M.pow(2)\n",
    "        self.entropy = sp.stats.entropy(flat_M_2.detach().numpy())\n",
    "            \n",
    "        return F.conv2d(input, masked_weight, self.bias, stride=self.stride, padding=self.padding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58dc0fc2-3272-47a0-aa79-aec511bdb7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 1.798\n",
      "Epoch 1, Batch 200, Loss: 1.441\n",
      "Epoch 1, Batch 300, Loss: 1.302\n",
      "Epoch 1, Batch 400, Loss: 1.213\n",
      "Epoch 1, Batch 500, Loss: 1.152\n",
      "Epoch 1, Batch 600, Loss: 1.114\n",
      "Epoch 1, Batch 700, Loss: 1.055\n",
      "Epoch 2, Batch 100, Loss: 0.914\n",
      "Epoch 2, Batch 200, Loss: 0.898\n",
      "Epoch 2, Batch 300, Loss: 0.885\n",
      "Epoch 2, Batch 400, Loss: 0.888\n",
      "Epoch 2, Batch 500, Loss: 0.878\n",
      "Epoch 2, Batch 600, Loss: 0.872\n",
      "Epoch 2, Batch 700, Loss: 0.825\n",
      "Epoch 3, Batch 100, Loss: 0.706\n",
      "Epoch 3, Batch 200, Loss: 0.677\n",
      "Epoch 3, Batch 300, Loss: 0.677\n",
      "Epoch 3, Batch 400, Loss: 0.706\n",
      "Epoch 3, Batch 500, Loss: 0.720\n",
      "Epoch 3, Batch 600, Loss: 0.687\n",
      "Epoch 3, Batch 700, Loss: 0.681\n",
      "Test Accuracy: 73.28%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "#model_A = CNN(MyActivation(2)).to(device)\n",
    "model = CNN(nn.ReLU, use_mask=False).to(device)\n",
    "the_lambda = 1e-6\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader, 0):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        if the_lambda > 0:\n",
    "            norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss += the_lambda*norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d24a5b9e-d78e-4172-99e3-904363221aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float32(6.9278994), np.float32(9.817288), np.float32(13.1984215), np.float32(7.493392)]\n"
     ]
    }
   ],
   "source": [
    "entropies = []\n",
    "layer_count = 0\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, (nn.Conv2d, MaskedLinear, nn.Linear, MaskedConv2d)):\n",
    "        layer_count += 1\n",
    "        #print(layer, layer.entropy)\n",
    "        entropies.append(layer.entropy)\n",
    "\n",
    "#print(f\"Epoch {epoch + 1}, Weight Entropies: {entropies}\")\n",
    "print(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c8ed6-9873-46ae-9eb3-27468d3e3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac516429-1c45-4680-b4e5-5597a2465725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_conv_weights(layer, title=\"Conv Filters\", max_filters=64):\n",
    "    weights = layer.weight.data.cpu().numpy()  # Shape: (out_channels, in_channels, H, W)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "\n",
    "    n_filters = min(weights.shape[0], max_filters)\n",
    "    ncols = 8\n",
    "    nrows = (n_filters + ncols - 1) // ncols\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(n_filters):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        # Collapse to RGB by transposing\n",
    "        img = weights[i].transpose(1, 2, 0)  # (H, W, C)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "    for i in range(n_filters, nrows * ncols):\n",
    "        axes[i // ncols, i % ncols].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716ed71-5f63-4ddc-9fc4-0dc33b07bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_conv_weights(model.model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dbaeab-bcf7-49ab-8965-ef79fb6995e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight_distribution(layer, title=\"Weight Magnitude Histogram\"):\n",
    "    weights = layer.weight.data.cpu().numpy().flatten()\n",
    "    plt.hist(weights, bins=100)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Weight value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_weight_distribution(model.model[0], \"First Conv Layer\")\n",
    "plot_weight_distribution(model.model[-1], \"Final Linear Layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13b8f3-dbc4-4386-9822-2118985bf7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
